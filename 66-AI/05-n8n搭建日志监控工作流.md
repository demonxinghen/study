1.安装Python，3.13以上吧

2.安装nodejs，22以上吧

3.安装n8n
```shell
pip install n8n
```

4.启动n8n
```shell
npx n8n
```

5.设置创建者Set up owner account,随便写,记住就行

admin@qq.com

admin

admin

kbqyknm6@H@CRSK

6.创建之后，进去的弹窗直接关掉，我们是自建的，不需要那些

7.直接创建工作流，create workflow

8.add first step,选择一个trigger,监控日志,我们要定时执行,选择On a schedule,然后自定义触发间隔

9.连接服务器,创建ssh节点->Execute a command,配置ssh信息,先创建一个Credential,保存后如果可以连上服务器,会显示Connection tested successfully
```text
Host: IP
Port: 端口
Username:
Private Key: 私钥,记住要把-----BEGIN OPENSSH PRIVATE KEY-----和-----END OPENSSH PRIVATE KEY-----一起复制进去
Passphrase: 没有则不填
```
再配置Command和Working Directory
```text
Command: LATEST_LOG=$(ls -t *.log 2>/dev/null | head -1); if [ -f "$LATEST_LOG" ]; then echo "FILENAME:$LATEST_LOG"; LAST_POS_FILE="/tmp/n8n_log_position_${USER}"; if [ -f "$LAST_POS_FILE" ]; then LAST_POS=$(cat "$LAST_POS_FILE"); else LAST_POS=0; fi; NEW_LINES=$(tail -c +$((LAST_POS + 1)) "$LATEST_LOG" | grep -E '^[0-9]{2}:[0-9]{2}:[0-9]{2}\.[0-9]{3}' | head -1000); if [ ! -z "$NEW_LINES" ]; then echo "$NEW_LINES"; fi; wc -c < "$LATEST_LOG" > "$LAST_POS_FILE"; else echo 'No log files found'; fi
Working Directory: 工作目录也就是我们日志的目录
```
Command解析,按照目前我的理解,命令中echo的内容会进入到传输给下一节点,所以echo "FILENAME:$LATEST_LOG"是方便直接知道出错日志
```shell
# 查找最新的日志文件
LATEST_LOG=$(ls -t *.log 2>/dev/null | head -1)
# 判断日志文件是否存在
if [ -f "$LATEST_LOG" ]; then
    # 输出日志文件名
    echo "FILENAME:$LATEST_LOG"

    # 读取上次读取的位置
    LAST_POS_FILE="/tmp/n8n_log_position_${USER}"

    # 判断上次读取的位置文件,如果存在,读取偏移量,否则从头开始读取
    if [ -f "$LAST_POS_FILE" ]; then
        LAST_POS=$(cat "$LAST_POS_FILE")
    else
        LAST_POS=0
    fi

    # 读取新的日志行,使用tail命令读取指定偏移量之后的内容,使用grep过滤出以时间戳开头的行,使用head限制输出行数
    NEW_LINES=$(tail -c +$((LAST_POS + 1)) "$LATEST_LOG" | grep -E '^[0-9]{2}:[0-9]{2}:[0-9]{2}\.[0-9]{3}' | head -1000)

    # 判断是否有新的日志行,如果有,输出日志行
    if [ ! -z "$NEW_LINES" ]; then
        echo "$NEW_LINES"
    fi

    # 更新上次读取的位置,使用wc命令统计文件大小,并将结果写入上次读取的位置文件
    wc -c < "$LATEST_LOG" > "$LAST_POS_FILE"
else
    echo 'No log files found'
fi
```
10.解析日志,新建Code节点,这里我们使用js语言
```javascript
// Parse log lines and extract relevant information
const logOutput = $input.first().json.stdout;

if (!logOutput || logOutput.trim() === '' || logOutput.trim() === 'No log files found') {
  console.log('No new log lines found');
  return [];
}

const lines = logOutput.split('\n').filter(line => line.trim() !== '');
const parsedLogs = [];
console.log(`Processing ${lines.length} new log lines`);
let date = "";
let filename = "";

for (let i = 0; i < lines.length; i++) {
  const line = lines[i];

  if (line.startsWith("filename")) {
    filename = line.split(':')[1];
    const match = filename.match(/\d{4}-\d{2}-\d{2}/);
    if (match) {
      date = match[0];
    }
    continue;
  }

  const timestampMatch = line.match(/^(\d{2}:\d{2}:\d{2}\.\d{3})/);

  if (timestampMatch) {
    const timestamp = timestampMatch[1];
    let logContent = line.substring(timestamp.length).trim();
    
    const containsConsumeFailed = /消费失败,失败原因/.test(logContent) && !logContent.includes('重复消费');
    const containsOverseasOrderFailed = /海外仓订单新增失败/.test(logContent) && logContent.includes('返回结果');

    let errorType = 0;
    let clientOrderId = "";
    let message = "";
    let originalCustomerId = "未知";
    let parameters = null;
    let result = null;

    // 由于rocketmq会重试消费,所以会一直通知,一开始在这里根据重试次数来判断是否要触发告警,后面直接把这个做到了项目切面里,这里就不用了
    // const match = /reconsumeTimes:(1|7|15)(?!\d)/.test(logContent);

    if (containsConsumeFailed) {
      errorType = 1;
      logContent = logContent.replaceAll("	at", "	\nat");
    } else if (containsOverseasOrderFailed) {
      errorType = 2;

      // ---- 提取参数 JSON ----
      const paramStart = logContent.indexOf('参数:');
      const resultStart = logContent.indexOf('返回结果:');

      if (paramStart !== -1 && resultStart !== -1) {
        const paramSection = logContent.slice(paramStart + 3, resultStart);
        const firstBrace = paramSection.indexOf('{');
        let braceCount = 0;
        let endIndex = -1;

        for (let k = firstBrace; k < paramSection.length; k++) {
          const ch = paramSection[k];
          if (ch === '{') braceCount++;
          else if (ch === '}') braceCount--;
          if (braceCount === 0) {
            endIndex = k;
            break;
          }
        }

        if (firstBrace !== -1 && endIndex !== -1) {
          const paramJsonStr = paramSection.slice(firstBrace, endIndex + 1);
          try {
            parameters = JSON.parse(paramJsonStr);
          } catch (e) {
            console.log('参数 JSON 解析失败:', e);
          }
        }

        // ---- 提取返回结果 JSON ----
        const resultSection = logContent.slice(resultStart + 5);
        const firstBraceR = resultSection.indexOf('{');
        braceCount = 0;
        endIndex = -1;

        for (let k = firstBraceR; k < resultSection.length; k++) {
          const ch = resultSection[k];
          if (ch === '{') braceCount++;
          else if (ch === '}') braceCount--;
          if (braceCount === 0) {
            endIndex = k;
            break;
          }
        }

        if (firstBraceR !== -1 && endIndex !== -1) {
          const resultJsonStr = resultSection.slice(firstBraceR, endIndex + 1);
          try {
            result = JSON.parse(resultJsonStr);
          } catch (e) {
            console.log('返回结果 JSON 解析失败:', e);
          }
        }

        // ---- 从解析结果中获取字段 ----
        clientOrderId = parameters?.clientOrderId || "";
        originalCustomerId = parameters?.originalCustomerId || "未知";
        message = result?.message?.replace("海外仓订单新增失败：", "") || "";
      }
    }

    parsedLogs.push({
      timestamp: date + " " + timestamp, 
      fullLine: line,
      content: logContent,
      triggerAlert: errorType !== 0,
      errorType: errorType,
      filename: filename,
      originalCustomerId: originalCustomerId,
      clientOrderId: clientOrderId,
      message: message
      // parameters: parameters,
      // result: result
    });
  }
}

console.log(`Found ${parsedLogs.filter(log => log.triggerAlert).length} alert lines out of ${parsedLogs.length} total lines`);
return parsedLogs.map(log => ({ json: log }));
```
11.新建Switch节点(也可以用if,但是if的结果只有两个分支,switch可以有多个分支),根据errorType的值来选择不同的分支,这里我们有三种类型的错误,所以有三个分支

这里的Mode可以选择Rules,也可以选择Expression

Rules: 比较简单,就是根据errorType的值来选择不同的分支

Expression: 需要设置Numbers of Outputs, 这里我们应该设置为3, 然后设置Output Index的字段, 把errorType放进去, 然后0就会走后面第一个分支线, 1就会走第二个分支线, 2就会走第三个分支线, 比较类似于根据索引值走分支, 不用写具体的比较条件

我选择的是Expression, 然后设置Numbers of Outputs为3, Output Index的字段为errorType

0就不用处理了,因为是正常日志,要处理的话就加一个node节点
```javascript
// Log normal processing
const inputData = $input.all();

if (inputData.length > 0) {
  console.log(`Processed ${inputData.length} log lines without alerts`);
  
  return {
    json: {
      alertTriggered: false,
      processedLines: inputData.length,
      timestamp: new Date().toISOString(),
      message: 'Log monitoring completed - no alerts triggered'
    }
  };
} else {
  return {
    json: {
      alertTriggered: false,
      processedLines: 0,
      timestamp: new Date().toISOString(),
      message: 'No new log lines found'
    }
  };
}
```

1就是消费失败

2是海外仓订单新增失败

12.新建HTTP Request节点,用来通知企微
```text
Method: POST
URL: 企微机器人地址
Authentication: None
Send Body: JSON
Specify Body: Using JSON
JSON: {
  "msgtype": "text",
  "text": {
    "content": "时间: {{ $json.timestamp }}\n文件名: {{ $json.filename }}异常消息: {{ JSON.stringify($json.message).slice(1, -1) }}"
  }
}
```
这样就可以了。使用JSON.stringify($json.message).slice(1, -1)是为了防止日志中出现大量的双引号,会导致json解析失败

13.接入自建数据库(Postgresql)

默认存储的是sqlite,如果需要使用Postgresql(官方推荐数据库)

创建数据库
```postgresql
CREATE DATABASE n8n;
CREATE USER n8n_user WITH PASSWORD '123456';
-- 授予权限,否则启动n8n时会提示权限不足
ALTER DATABASE n8n OWNER TO n8n_user;
```

安装好数据库后,修改n8n的配置文件,在.env文件(默认读取启动命令窗口下的.env)中添加以下配置
```text
# PostgreSQL 数据库配置
DB_TYPE=postgresdb
DB_POSTGRESDB_HOST=localhost
DB_POSTGRESDB_PORT=5432
DB_POSTGRESDB_DATABASE=n8n
DB_POSTGRESDB_USER=n8n_user
DB_POSTGRESDB_PASSWORD=123456

# 可选：关闭自动检查版本提示,加上这个是因为每次启动时,如果n8n有更新,就会提示你更新
N8N_VERSION_NOTIFICATIONS_ENABLED=false

# 禁用执行记录自动清理,n8n会自动清理执行记录
N8N_EXECUTIONS_PRUNING_ENABLED=false

# 自定义清理策略：只删除30天前的记录，每30天清理一次
N8N_EXECUTIONS_PRUNING_ENABLED=true
N8N_EXECUTIONS_PRUNING_MAX_AGE=30
N8N_EXECUTIONS_PRUNING_TIMEOUT=720
```
14.查询指定时间范围内的执行记录
```postgresql
-- 查询指定日期执行的工作流（例如 2025-07-09）,postgresql数据库列名区分大小写
SELECT id, "workflowId", status, "startedAt"
FROM execution_entity
WHERE "startedAt" >= '2025-07-09 00:00:00'
  AND "startedAt" < '2025-07-10 00:00:00'
ORDER BY "startedAt" DESC
LIMIT 50;
```

15.获取默认字段
```text
$execution.id	当前执行 ID
$workflow.id	当前工作流 ID
$workflow.name	当前工作流名称
$now	当前时间戳（ISO 字符串）
$json	当前数据项（JSON 对象）
```